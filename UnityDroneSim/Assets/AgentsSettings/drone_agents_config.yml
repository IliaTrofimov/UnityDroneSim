behaviors:
  # --------------------------------------
  # Drone with depth camera sensor
  # --------------------------------------
  Drone_DepthSensor_1:
    trainer_type: ppo       # - ppo | sac | poca
    max_steps: 5000000      # - Number of steps (observation collected & action taken) before ending the training.
    time_horizon: 86        # - How many steps of experience to collect per-agent before adding it to the experience buffer.
                            #   When this limit is reached before the end of an episode, a value estimate is used 
                            #   to predict the overall expected reward from the agent's current state.
                            #   As such, this parameter trades off between a less biased, but higher variance estimate 
                            #   (long time horizon) and more biased, but less varied estimate (short time horizon). 
                            #   In cases where there are frequent rewards within an episode, or episodes are prohibitively large, 
                            #   a smaller number can be more ideal. This number should be large enough to capture all the important behavior
                            #   within a sequence of an agent's actions.
                            #   Typically from 32 to 2048.
    keep_checkpoints: 3
    checkpoint_interval: 10000
    summary_freq: 5000
    
    hyperparameters:
      batch_size: 100         # - Number of experiences in each iteration of gradient descent. This should always be multiple times smaller than buffer_size.
                              #   Typical range: Continuous PPO: 512 - 5120; Continuous SAC: 128 - 1024; Discrete: 32 - 512.
      buffer_size: 1024       # - PPO: Number of experiences to collect before updating the policy model. 
                              #   Corresponds to how many experiences should be collected before we do any learning or updating of the model. 
                              #   Typical range: PPO: 2048 - 409600
      learning_rate: 3.0e-4   # - Initial learning rate for gradient descent. Corresponds to the strength of each gradient descent update step. 
                              #   This should typically be decreased if training is unstable, and the reward does not consistently increase.
                              #   Typical range: 10^-5 - 10^-3
      learning_rate_schedule: linear
      # -------- PPO specific --------
      beta: 3.0e-4            # - Strength of the entropy regularization, which makes the policy "more random." 
                              #   This ensures that agents properly explore the action space during training. Increasing this 
                              #   will ensure more random actions are taken. This should be adjusted such that the entropy 
                              #   (measurable from TensorBoard) slowly decreases alongside increases in reward. 
                              #   If entropy drops too quickly, increase beta. If entropy drops too slowly, decrease beta.
                              #   Typical range: 1e-4 - 1e-2
      epsilon: 0.15           # - Influences how rapidly the policy can evolve during training. Corresponds to the acceptable
                              #   threshold of divergence between the old and new policies during gradient descent updating. 
                              #   Setting this value small will result in more stable updates, but will also slow the training process.
                              #   Typical range: 0.1 - 0.3
      lambd: 0.99             # - Regularization parameter used when calculating the Generalized Advantage Estimate (GAE).
                              #   This can be thought of as how much the agent relies on its current value estimate when 
                              #   calculating an updated value estimate. Low values correspond to relying more on the 
                              #   current value estimate (which can be high bias), and high values correspond to relying more 
                              #   on the actual rewards received in the environment (which can be high variance).
                              #   The parameter provides a trade-off between the two, and the right value can lead to a more stable training process.
                              #   Typical range: 0.9 - 0.95
      num_epoch: 3            # - Number of passes to make through the experience buffer when performing gradient descent optimization.
                              #   The larger the batch_size, the larger it is acceptable to make this. Decreasing this will 
                              #   ensure more stable updates, at the cost of slower learning.
                              #   Typical range: 3 - 10
      beta_schedule: linear
      epsilon_schedule: linear
    
    network_settings:
      normalize: true       # - Whether normalization is applied to the vector observation inputs. This normalization is
                            #   based on the running average and variance of the vector observation. Normalization can be 
                            #   helpful in cases with complex continuous control problems, but may be harmful with simpler discrete control problems.
      num_layers: 4         # - Corresponds to how many hidden layers are present after the observation input, 
                            #   or after the CNN encoding of the visual observation. For simple problems, fewer layers are likely 
                            #   to train faster and more efficiently. More layers may be necessary for more complex control problems.
                            #   Typical range: 1 - 3
      hidden_units: 64      # - Correspond to how many units are in each fully connected layer of the neural network. 
                            #   For simple problems where the correct action is a straightforward combination of the observation inputs,
                            #   this should be small. For problems where the action is a very complex interaction between 
                            #   the observation variables, this should be larger.
                            #   Typical range: 32 - 512
      conditioning_type: none
      vis_encode_type: simple
    
    reward_signals:
      extrinsic:
        gamma: 0.90         # - Discount factor for future rewards coming from the environment. This can be thought of
                            #   as how far into the future the agent should care about possible rewards. In situations 
                            #   when the agent should be acting in the present in order to prepare for rewards in the distant future,
                            #   this value should be large. In cases when rewards are more immediate, it can be smaller.
                            #   Must be strictly smaller than 1.
        strength: 1.0
        
    
  # --------------------------------------
  # Drone with 2 grid sensors (forward 3d sensor and top-down 2d)
  # --------------------------------------
  Drone_GridSensor_1:
    trainer_type: ppo
    max_steps: 5000000
    time_horizon: 86
    keep_checkpoints: 3
    checkpoint_interval: 250000
    summary_freq: 20000

    hyperparameters:
      batch_size: 10
      buffer_size: 100
      learning_rate: 3.0e-4
      learning_rate_schedule: linear
      beta: 5.0e-4
      epsilon: 0.2
      lambd: 0.99
      num_epoch: 3
      beta_schedule: constant
      epsilon_schedule: linear

    network_settings:
      normalize: false
      num_layers: 2
      hidden_units: 128
      conditioning_type: none

    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0    
  
  # --------------------------------------
  # Drone with raycast sensors
  # --------------------------------------
  Drone_Raycaster_1:
    trainer_type: ppo
    max_steps: 5000000
    time_horizon: 128
    keep_checkpoints: 3
    checkpoint_interval: 250000
    summary_freq: 20000

    hyperparameters:
      batch_size: 128
      buffer_size: 2048
      learning_rate: 3.0e-4
      learning_rate_schedule: linear
      beta: 5.5e-3
      epsilon: 0.2
      lambd: 0.90
      num_epoch: 3
      beta_schedule: constant
      epsilon_schedule: linear

    network_settings:
      normalize: false
      num_layers: 3
      hidden_units: 64
      conditioning_type: none

    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
  
  # --------------------------------------
  # Drone with raycast sensors (more raycasting sensors) 
  # --------------------------------------
  Drone_Raycaster_2:
    trainer_type: ppo
    max_steps: 10000000
    time_horizon: 86
    keep_checkpoints: 1
    checkpoint_interval: 250000
    summary_freq: 20000
    threaded: true
    
    hyperparameters:
      batch_size: 128
      buffer_size: 12800
      learning_rate: 2.5e-4
      learning_rate_schedule: linear
      beta: 3.0e-4
      epsilon: 0.25
      lambd: 0.96
      num_epoch: 4
      beta_schedule: linear
      epsilon_schedule: linear
    
    network_settings:
      normalize: false
      num_layers: 4
      hidden_units: 86
      conditioning_type: none

    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
